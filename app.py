import streamlit as st
import pandas as pd
from openpyxl import load_workbook
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment
import datetime
import io
import re
import html
import numpy as np

# --- Configuraci√≥n de la p√°gina ---
st.set_page_config(page_title="Procesador de Dossiers (Lite) v1.7", layout="wide")

# ==============================================================================
# SECCI√ìN DE FUNCIONES AUXILIARES
# ==============================================================================

def extract_link_from_cell(cell):
    if cell.hyperlink and cell.hyperlink.target:
        return cell.hyperlink.target
    return None

def convert_html_entities(text):
    """
    Convierte entidades HTML mal codificadas a caracteres normales.
    Maneja tanto entidades hexadecimales como decimales.
    """
    if not isinstance(text, str):
        return text
    
    # Primero decodificar entidades HTML est√°ndar
    text = html.unescape(text)

    # Manejar entidades HTML num√©ricas hexadecimales espec√≠ficas
    html_entities = {
        '&#xF3;': '√≥',  # √≥
        '&#xE1;': '√°',  # √°
        '&#xE9;': '√©',  # √©
        '&#xED;': '√≠',  # √≠
        '&#xFA;': '√∫',  # √∫
        '&#xF1;': '√±',  # √±
        '&#xDC;': '√ú',  # √ú
        '&#xFC;': '√º',  # √º
        '&#xC1;': '√Å',  # √Å
        '&#xC9;': '√â',  # √â
        '&#xCD;': '√ç',  # √ç
        '&#xD3;': '√ì',  # √ì
        '&#xDA;': '√ö',  # √ö
        '&#xD1;': '√ë',  # √ë
        '&#xC7;': '√á',  # √á
        '&#xE7;': '√ß',  # √ß
    }

    # Reemplazar entidades HTML num√©ricas hexadecimales
    for entity, char in html_entities.items():
        text = text.replace(entity, char)

    # Patr√≥n para capturar otras entidades hexadecimales que no est√©n en el diccionario
    def replace_hex_entity(match):
        try:
            hex_code = match.group(1)
            char_code = int(hex_code, 16)
            return chr(char_code)
        except (ValueError, OverflowError):
            return match.group(0)  # Devolver original si no se puede convertir

    text = re.sub(r'&#x([0-9A-Fa-f]+);', replace_hex_entity, text)

    # Patr√≥n para entidades decimales
    def replace_decimal_entity(match):
        try:
            decimal_code = int(match.group(1))
            return chr(decimal_code)
        except (ValueError, OverflowError):
            return match.group(0)  # Devolver original si no se puede convertir

    text = re.sub(r'&#(\d+);', replace_decimal_entity, text)

    # Limpiar caracteres problem√°ticos adicionales
    custom_replacements = {
        '"': '"',
        '"': '"', 
        ''': "'", 
        ''': "'", 
        '√Ç': '', 
        '√¢': '', 
        '‚Ç¨': '', 
        '‚Ñ¢': ''
    }

    for entity, char in custom_replacements.items():
        text = text.replace(entity, char)

    return text

def normalize_title_for_comparison(title):
    """
    Normaliza el t√≠tulo para comparaci√≥n de duplicados.
    """
    if not isinstance(title, str):
        return ""
    
    # Limpiar entidades HTML primero
    title = convert_html_entities(title)

    # Normalizar para comparaci√≥n (remover caracteres especiales y convertir a min√∫sculas)
    return re.sub(r'\W+', ' ', title).lower().strip()

def clean_title_for_output(title):
    """
    √öNICAMENTE limpia entidades HTML mal codificadas.
    NO corta, NO modifica, NO remueve ninguna parte del t√≠tulo.
    Solo convierte caracteres como √≥ a √≥
    """
    if not isinstance(title, str):
        return ""
    
    # SOLO limpiar entidades HTML - NO tocar nada m√°s
    title = convert_html_entities(title)

    # Solo quitar espacios al inicio y final, NO espacios m√∫ltiples internos
    title = title.strip()

    return title

def corregir_texto(text):
    if not isinstance(text, str): 
        return text
    text = convert_html_entities(text)
    text = re.sub(r'(<br>|[...]|\s+)', ' ', text).strip()
    match = re.search(r'[A-Z]', text)
    if match: 
        text = text[match.start():]
    if text and not text.endswith('...'): 
        text = text.rstrip('.') + '...'
    return text

def to_excel_from_df(df, final_order):
    """
    Genera archivo Excel usando openpyxl para evitar la limitaci√≥n de 64k enlaces de xlsxwriter.
    Esta versi√≥n no tiene l√≠mite en la cantidad de hiperv√≠nculos.
    """
    output = io.BytesIO()
    
    # Filtrar columnas que existen en el DataFrame
    final_columns_in_df = [col for col in final_order if col in df.columns]
    df_to_excel = df[final_columns_in_df]
    
    # Crear workbook con openpyxl
    wb = Workbook()
    ws = wb.active
    ws.title = 'Resultado'
    
    # Escribir encabezados con formato
    for col_idx, col_name in enumerate(df_to_excel.columns, start=1):
        cell = ws.cell(row=1, column=col_idx, value=col_name)
        cell.font = Font(bold=True)
    
    # Identificar columnas de enlaces
    link_columns = set()
    for col_name in ['Link Nota', 'Link (Streaming - Imagen)']:
        if col_name in df_to_excel.columns:
            link_columns.add(col_name)
    
    # Escribir datos fila por fila
    for row_idx, row_data in enumerate(df_to_excel.itertuples(index=False), start=2):
        for col_idx, value in enumerate(row_data, start=1):
            col_name = df_to_excel.columns[col_idx - 1]
            cell = ws.cell(row=row_idx, column=col_idx)
            
            # Manejar fechas
            if col_name == 'Fecha' and pd.notna(value):
                if isinstance(value, pd.Timestamp):
                    cell.value = value.to_pydatetime()
                    cell.number_format = 'DD/MM/YYYY'
                else:
                    cell.value = value
            # Crear hiperv√≠nculos para columnas de enlaces
            elif col_name in link_columns and pd.notna(value) and isinstance(value, str) and value.startswith('http'):
                cell.value = 'Link'
                cell.hyperlink = value
                cell.font = Font(color="0563C1", underline="single")
                cell.alignment = Alignment(horizontal='left')
            else:
                cell.value = value
    
    # Ajustar anchos de columna para mejor presentaci√≥n
    for col_idx, col_name in enumerate(df_to_excel.columns, start=1):
        if col_name in ['T√≠tulo', 'Resumen - Aclaracion']:
            ws.column_dimensions[ws.cell(row=1, column=col_idx).column_letter].width = 50
        elif col_name in ['Link Nota', 'Link (Streaming - Imagen)']:
            ws.column_dimensions[ws.cell(row=1, column=col_idx).column_letter].width = 15
        elif col_name == 'Mantener':
            ws.column_dimensions[ws.cell(row=1, column=col_idx).column_letter].width = 25
        else:
            ws.column_dimensions[ws.cell(row=1, column=col_idx).column_letter].width = 20
    
    # Guardar en BytesIO
    wb.save(output)
    output.seek(0)
    
    return output.getvalue()

# ==============================================================================
# L√ìGICA DE PROCESAMIENTO PRINCIPAL
# ==============================================================================

def run_full_process(dossier_file, config_file):
    
    st.markdown("---")
    progress_text = st.empty()

    progress_text.info("Paso 1/4: Cargando archivo de configuraci√≥n...")
    try:
        config_sheets = pd.read_excel(config_file.read(), sheet_name=None)
        region_map = pd.Series(config_sheets['Regiones'].iloc[:, 1].values, index=config_sheets['Regiones'].iloc[:, 0].astype(str).str.lower().str.strip()).to_dict()
        internet_map = pd.Series(config_sheets['Internet'].iloc[:, 1].values, index=config_sheets['Internet'].iloc[:, 0].astype(str).str.lower().str.strip()).to_dict()
    except Exception as e:
        st.error(f"Error al cargar `Configuracion.xlsx`: {e}. Aseg√∫rate de que contenga las hojas 'Regiones' e 'Internet'.")
        st.stop()

    progress_text.info("Paso 2/4: Leyendo Dossier y expandiendo filas...")
    wb = load_workbook(dossier_file)
    sheet = wb.active
    original_headers = [cell.value for cell in sheet[1] if cell.value]
    rows_to_expand = []
    for row in sheet.iter_rows(min_row=2):
        if all(c.value is None for c in row): 
            continue
        row_values = [c.value for c in row]
        row_data = dict(zip(original_headers, row_values))
        if 'Link Nota' in original_headers: 
            row_data['Link Nota'] = extract_link_from_cell(row[original_headers.index('Link Nota')])
        if 'Link (Streaming - Imagen)' in original_headers: 
            row_data['Link (Streaming - Imagen)'] = extract_link_from_cell(row[original_headers.index('Link (Streaming - Imagen)')])
        menciones = [m.strip() for m in str(row_data.get('Menciones - Empresa') or '').split(';') if m.strip()]
        if not menciones: 
            rows_to_expand.append(row_data)
        else:
            for mencion in menciones:
                new_row = row_data.copy()
                new_row['Menciones - Empresa'] = mencion
                rows_to_expand.append(new_row)
    df = pd.DataFrame(rows_to_expand)
    df['Estado_Duplicado'] = 'Conservar'

    progress_text.info("Paso 3/4: Aplicando limpieza, mapeos y normalizaciones...")
    for col in original_headers:
        if col not in df.columns: 
            df[col] = None

    # APLICAR LA LIMPIEZA CORREGIDA - SOLO ENTIDADES HTML, NO CORTAR
    df['T√≠tulo'] = df['T√≠tulo'].astype(str).apply(clean_title_for_output)
    df['Resumen - Aclaracion'] = df['Resumen - Aclaracion'].astype(str).apply(corregir_texto)

    tipo_medio_map = {'online': 'Internet', 'diario': 'Prensa', 'am': 'Radio', 'fm': 'Radio', 'aire': 'Televisi√≥n', 'cable': 'Televisi√≥n', 'revista': 'Revista'}
    df['Tipo de Medio'] = df['Tipo de Medio'].str.lower().str.strip().map(tipo_medio_map).fillna(df['Tipo de Medio'])
    is_internet = df['Tipo de Medio'] == 'Internet'
    is_print = df['Tipo de Medio'].isin(['Prensa', 'Revista'])
    is_broadcast = df['Tipo de Medio'].isin(['Radio', 'Televisi√≥n'])

    df.loc[is_internet, ['Link Nota', 'Link (Streaming - Imagen)']] = df.loc[is_internet, ['Link (Streaming - Imagen)', 'Link Nota']].values
    cond_copy = is_print & df['Link Nota'].isnull() & df['Link (Streaming - Imagen)'].notnull()
    df.loc[cond_copy, 'Link Nota'] = df.loc[cond_copy, 'Link (Streaming - Imagen)']
    df.loc[is_print, 'Link (Streaming - Imagen)'] = None
    df.loc[is_broadcast, 'Link (Streaming - Imagen)'] = None

    # --- INICIO DE LA L√ìGICA "CORTAR Y PEGAR" ---
    if 'Duraci√≥n - Nro. Caracteres' in df.columns and 'Dimensi√≥n' in df.columns:
        # 1. Copiar el valor a la columna Dimensi√≥n para medios broadcast
        df.loc[is_broadcast, 'Dimensi√≥n'] = df.loc[is_broadcast, 'Duraci√≥n - Nro. Caracteres']
        # 2. Limpiar (cortar) el valor de la columna original para esos mismos medios
        df.loc[is_broadcast, 'Duraci√≥n - Nro. Caracteres'] = np.nan
    # --- FIN DE LA L√ìGICA "CORTAR Y PEGAR" ---

    df['Regi√≥n'] = df['Medio'].astype(str).str.lower().str.strip().map(region_map)
    df.loc[is_internet, 'Medio'] = df.loc[is_internet, 'Medio'].astype(str).str.lower().str.strip().map(internet_map).fillna(df.loc[is_internet, 'Medio'])

    progress_text.info("Paso 4/4: Detectando duplicados y generando resultados...")
    df['titulo_norm'] = df['T√≠tulo'].apply(normalize_title_for_comparison)
    df['Fecha'] = pd.to_datetime(df['Fecha'], dayfirst=True, errors='coerce').dt.normalize()

    df['seccion_priority'] = df['Secci√≥n - Programa'].isnull() | (df['Secci√≥n - Programa'] == '')
    df['dup_hora'] = np.where(df['Tipo de Medio'] == 'Internet', 'IGNORE_TIME', df['Hora'])

    # Crear columna Mantener inicialmente vac√≠a
    df['Mantener'] = ''

    dup_cols_exact = ['titulo_norm', 'Medio', 'Fecha', 'Menciones - Empresa', 'dup_hora']
    sort_by_cols = dup_cols_exact + ['seccion_priority']
    ascending_order = [True] * len(dup_cols_exact) + [False]
    df.sort_values(by=sort_by_cols, ascending=ascending_order, inplace=True)
    
    # Identificar duplicados exactos y guardar el ID de la noticia a mantener
    for name, group in df.groupby(dup_cols_exact):
        if len(group) > 1:
            # El primero es el que se mantiene (tiene mejor secci√≥n)
            id_to_keep = group.iloc[0]['ID Noticia']
            # Los dem√°s son duplicados
            duplicate_indices = group.index[1:]
            df.loc[duplicate_indices, 'Estado_Duplicado'] = 'Eliminar'
            df.loc[duplicate_indices, 'Mantener'] = f'Duplicado de: {id_to_keep}'
    
    df.sort_index(inplace=True)

    # Procesar duplicados consecutivos de Internet
    df_internet_to_check = df[(df['Estado_Duplicado'] == 'Conservar') & (is_internet)].copy()
    if not df_internet_to_check.empty:
        group_cols = ['titulo_norm', 'Medio', 'Menciones - Empresa']
        df_internet_to_check.sort_values(by=group_cols + ['Fecha'], inplace=True)
        date_diffs = df_internet_to_check.groupby(group_cols)['Fecha'].diff().dt.days
        cluster_ids = (date_diffs != 1).cumsum()
        df_internet_to_check['date_cluster'] = cluster_ids
        sort_by_cols_consecutive = group_cols + ['date_cluster', 'seccion_priority']
        ascending_order_consecutive = [True] * (len(group_cols) + 1) + [False]
        df_internet_to_check.sort_values(by=sort_by_cols_consecutive, ascending=ascending_order_consecutive, inplace=True)
        
        # Identificar duplicados consecutivos y guardar el ID de la noticia a mantener
        for name, group in df_internet_to_check.groupby(group_cols + ['date_cluster']):
            if len(group) > 1:
                id_to_keep = group.iloc[0]['ID Noticia']
                duplicate_indices = group.index[1:]
                df.loc[duplicate_indices, 'Estado_Duplicado'] = 'Eliminar'
                df.loc[duplicate_indices, 'Mantener'] = f'Duplicado de: {id_to_keep}'

    # Marcar las columnas Tono, Tema, Temas Generales como Duplicada
    df.loc[df['Estado_Duplicado'] == 'Eliminar', ['Tono', 'Tema', 'Temas Generales - Tema']] = 'Duplicada'

    st.balloons()
    progress_text.success("¬°Proceso de limpieza completado! Todas las duplicadas muestran el ID de la noticia a conservar.")

    final_order = ["ID Noticia", "Mantener", "Fecha", "Hora", "Medio", "Tipo de Medio", "Secci√≥n - Programa", "Regi√≥n", "T√≠tulo", "Autor - Conductor", "Nro. Pagina", "Dimensi√≥n", "Duraci√≥n - Nro. Caracteres", "CPE", "Tier", "Audiencia", "Tono", "Tema", "Temas Generales - Tema", "Resumen - Aclaracion", "Link Nota", "Link (Streaming - Imagen)", "Menciones - Empresa"]
    df_final = df.copy()

    st.subheader("üìä Resumen del Proceso")
    col1, col2, col3 = st.columns(3)
    col1.metric("Filas Totales", len(df_final))
    dups_count = (df_final['Estado_Duplicado'] == 'Eliminar').sum()
    col2.metric("Filas Marcadas como Duplicadas", dups_count)
    col3.metric("Filas √önicas", len(df_final) - dups_count)

    excel_data = to_excel_from_df(df_final, final_order)
    st.download_button(
        label="üì• Descargar Archivo Limpio y Mapeado", 
        data=excel_data, 
        file_name=f"Dossier_Limpio_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.xlsx", 
        mime="application/vnd.openxmlformats-officedocument.sheet"
    )

# ==============================================================================
# INTERFAZ PRINCIPAL DE STREAMLIT
# ==============================================================================

st.title("üöÄ Procesador de Dossiers (Lite) v1.7")
st.markdown("Una herramienta para limpiar, deduplicar y mapear dossieres de noticias.")
st.info("**Instrucciones:**\n\n1. Prepara tu archivo Dossier principal y tu archivo Configuracion.xlsx.\n2. Sube ambos archivos juntos en el √°rea de abajo.\n3. Haz clic en 'Iniciar Proceso'.\n4. La columna **Mantener** te indicar√° el ID de la noticia original cuando haya duplicados.")

# Informaci√≥n adicional sobre las mejoras
st.success("‚úÖ **T√≠tulos completos**: Solo se limpian entidades HTML como √≥ ‚Üí √≥")
st.success("‚úÖ **Sin l√≠mite de 64,000 hiperv√≠nculos**: Ahora se usa openpyxl")
st.success("‚úÖ **Columna Mantener**: Muestra 'Duplicado de: [ID]' para identificar cu√°l conservar")

with st.expander("Ver estructura requerida para Configuracion.xlsx"):
    st.markdown("- **Regiones**: Columna A (Medio), Columna B (Regi√≥n).\n- **Internet**: Columna A (Medio Original), Columna B (Medio Mapeado).")

uploaded_files = st.file_uploader(
    "Arrastra y suelta tus archivos aqu√≠ (Dossier y Configuracion)", 
    type=["xlsx"], 
    accept_multiple_files=True
)

dossier_file, config_file = None, None

if uploaded_files:
    for file in uploaded_files:
        if 'config' in file.name.lower(): 
            config_file = file
        else: 
            dossier_file = file
    
    if dossier_file: 
        st.success(f"‚úÖ Archivo Dossier cargado: {dossier_file.name}")
    else: 
        st.warning("‚ö†Ô∏è No se ha subido un archivo que parezca ser el Dossier.")
    
    if config_file: 
        st.success(f"‚úÖ Archivo de Configuraci√≥n cargado: {config_file.name}")
    else: 
        st.warning("‚ö†Ô∏è No se ha subido el archivo Configuracion.xlsx.")

if st.button(
    "‚ñ∂Ô∏è Iniciar Proceso de Limpieza", 
    disabled=not (dossier_file and config_file), 
    type="primary"
):
    run_full_process(dossier_file, config_file)
